{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.path.join(os.getcwd() , 'D:\\Projects\\google_ocr\\code\\My Project-b659c3bd30fa.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.cloud import storage\n",
    "    import google.cloud.storage\n",
    "    import json\n",
    "    import os\n",
    "    import sys\n",
    "except Exception as e:\n",
    "    print(\"Error : {} \".format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import io\n",
    "import tempfile\n",
    "import ghostscript\n",
    "import locale\n",
    "import glob\n",
    "import time\n",
    "\n",
    "from pydub import AudioSegment\n",
    "\n",
    "from google.cloud import storage\n",
    "from google.cloud import vision\n",
    "#from google.cloud import texttospeech\n",
    "# from google.cloud import automl_v1beta1 as automl\n",
    "from google.protobuf import json_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<google.cloud.storage.client.Client at 0x23498447288>"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "\n",
    "\n",
    "storage_client = storage.Client(PATH)\n",
    "\n",
    "storage_client\n"
   ]
  },
  {
   "source": [
    "!pip install --upgrade google-cloud-vision"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import io\n",
    "import os\n",
    "\n",
    "# Imports the Google Cloud client library\n",
    "from google.cloud import vision\n",
    "\n",
    "# Instantiates a client\n",
    "client = vision.ImageAnnotatorClient()\n",
    "\n",
    "# The name of the image file to annotate\n",
    "file_name = os.path.abspath(r'D:\\Projects\\google_ocr\\code\\Page_1.jpg')\n",
    "\n",
    "# Loads the image into memory\n",
    "with io.open(file_name, 'rb') as image_file:\n",
    "    content = image_file.read()\n",
    "\n",
    "image = vision.Image(content=content)\n",
    "\n",
    "# Performs label detection on the image file\n",
    "response = client.label_detection(image=image)\n",
    "labels = response.label_annotations\n",
    "\n",
    "print('Labels:')\n",
    "for label in labels:\n",
    "    print(label.description)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "def detect_text(path):\n",
    "    \"\"\"Detects text in the file.\"\"\"\n",
    "    from google.cloud import vision\n",
    "    import io\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "\n",
    "    with io.open(path, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = vision.Image(content=content)\n",
    "\n",
    "    response = client.text_detection(image=image)\n",
    "    texts = response.text_annotations\n",
    "    print('Texts:')\n",
    "\n",
    "    for text in texts:\n",
    "        print('\\n\"{}\"'.format(text.description))\n",
    "\n",
    "        vertices = (['({},{})'.format(vertex.x, vertex.y)\n",
    "                    for vertex in text.bounding_poly.vertices])\n",
    "\n",
    "        print('bounds: {}'.format(','.join(vertices)))\n",
    "\n",
    "    if response.error.message:\n",
    "        raise Exception(\n",
    "            '{}\\nFor more info on error messages, check: '\n",
    "            'https://cloud.google.com/apis/design/errors'.format(\n",
    "                response.error.message))"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "text=detect_text('D:\\Projects\\google_ocr\\code\\Page_1.jpg')"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['2020_Resume1.0.pdf',\n",
       " 'Resume1 output-1-to-1.json',\n",
       " '_Probability__and_Statistics___in___Data___Science___using___Python_Syllabus.pdf',\n",
       " 'resume.pdf']"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "\n",
    "bucket = storage_client.get_bucket('ocr_files_adil')\n",
    "\n",
    "filename = [filename.name for filename in list(bucket.list_blobs(prefix='')) ]\n",
    "filename\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = re.match(\"(.*).output-([0-9]+)-.*\",\"Resume1 output-1-to-1.json\")\n",
    "pdf_id = m.group(1)\n",
    "first_page = int(m.group(2))\n",
    "#json_blob = os.path.join(os.getcwd() , 'D:\\Projects\\google_ocr\\code\\My Project-b659c3bd30fa.json')\n",
    "json_blob=\"Resume1 output-1-to-1.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_csv(json_blob, pdf_id, first_page):\n",
    "    \n",
    "    # parse json\n",
    "    #json_string = json_blob.download_as_string()\n",
    "    json_string = bucket.blob(blob_name =json_blob).download_as_string()\n",
    "    json_response = json_format.Parse(json_string, vision.AnnotateFileResponse())\n",
    "\n",
    "    # covert the json file to a bag of CSV lines\n",
    "    csv = \"\"\n",
    "    page_count = first_page\n",
    "    for resp in json_response.responses:\n",
    "        para_count = 0\n",
    "        for page in resp.full_text_annotation.pages:\n",
    "\n",
    "            # collect para features for the page\n",
    "            page_features = []\n",
    "            for block in page.blocks:\n",
    "                if str(block.block_type) != \"1\":  # process only TEXT blocks\n",
    "                    continue\n",
    "                for para in block.paragraphs:\n",
    "                    para_id = \"{}-{:03}-{:03}\".format(pdf_id, page_count, para_count)\n",
    "                    f = extract_paragraph_feature(para_id, para)\n",
    "                    page_features.append(f)\n",
    "                    para_count += 1\n",
    "\n",
    "            # output to csv\n",
    "            for f in page_features:\n",
    "                csv += '{},\"{}\",{},{:.6f},{:.6f},{:.6f},{:.6f},{:.6f},{:.6f},{:.6f},{}\\n'.format(\n",
    "                    f[\"para_id\"],\n",
    "                    f[\"text\"],\n",
    "                    f[\"chars\"],\n",
    "                    f[\"width\"],\n",
    "                    f[\"height\"],\n",
    "                    f[\"area\"],\n",
    "                    f[\"char_size\"],\n",
    "                    f[\"pos_x\"],\n",
    "                    f[\"pos_y\"],\n",
    "                    f[\"aspect\"],\n",
    "                    f[\"layout\"],\n",
    "                )\n",
    "\n",
    "        page_count += 1\n",
    "    return csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_feature_csv(json_blob, pdf_id, first_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_string = bucket.blob(blob_name =json_blob).download_as_string()\n",
    "with open(json_string,\"wb\") as read_file:\n",
    "    data = json.load(read_file)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "blop = bucket.blob(blob_name =\"Resume1 output-1-to-1.json\").download_as_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def async_detect_document(gcs_source_uri, gcs_destination_uri):\n",
    "    \"\"\"OCR with PDF/TIFF as source files on GCS\"\"\"\n",
    "    import json\n",
    "    import re\n",
    "    from google.cloud import vision\n",
    "    from google.cloud import storage\n",
    "\n",
    "    # Supported mime_types are: 'application/pdf' and 'image/tiff'\n",
    "    mime_type = 'application/pdf'\n",
    "\n",
    "    # How many pages should be grouped into each json output file.\n",
    "    batch_size = 2\n",
    "\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "\n",
    "    feature = vision.Feature(\n",
    "        type_=vision.Feature.Type.DOCUMENT_TEXT_DETECTION)\n",
    "\n",
    "    gcs_source = vision.GcsSource(uri=gcs_source_uri)\n",
    "    input_config = vision.InputConfig(\n",
    "        gcs_source=gcs_source, mime_type=mime_type)\n",
    "\n",
    "    gcs_destination = vision.GcsDestination(uri=gcs_destination_uri)\n",
    "    output_config = vision.OutputConfig(\n",
    "        gcs_destination=gcs_destination, batch_size=batch_size)\n",
    "\n",
    "    async_request = vision.AsyncAnnotateFileRequest(\n",
    "        features=[feature], input_config=input_config,\n",
    "        output_config=output_config)\n",
    "\n",
    "    operation = client.async_batch_annotate_files(\n",
    "        requests=[async_request])\n",
    "\n",
    "    print('Waiting for the operation to finish.')\n",
    "    operation.result(timeout=420)\n",
    "\n",
    "    # Once the request has completed and the output has been\n",
    "    # written to GCS, we can list all the output files.\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    match = re.match(r'gs://([^/]+)/(.+)', gcs_destination_uri)\n",
    "    bucket_name = match.group(1)\n",
    "    prefix = match.group(2)\n",
    "\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "    # List objects with the given prefix.\n",
    "    blob_list = list(bucket.list_blobs(prefix=prefix))\n",
    "    print('Output files:')\n",
    "    for blob in blob_list:\n",
    "        print(blob.name)\n",
    "\n",
    "    # Process the first output file from GCS.\n",
    "    # Since we specified batch_size=2, the first response contains\n",
    "    # the first two pages of the input file.\n",
    "    output = blob_list[0]\n",
    "\n",
    "    json_string = output.download_as_string()\n",
    "    response = json.loads(json_string)\n",
    "    m = re.match(\"(.*).output-([0-9]+)-.*\",\"Resume1 output-1-to-1.json\")\n",
    "    pdf_id = m.group(1)\n",
    "    first_page = int(m.group(2))\n",
    "    json_blob=\"Resume1 output-1-to-1.json\"\n",
    "    \n",
    "    # The actual response for the first page of the input file.\n",
    "    first_page_response = response['responses'][0]\n",
    "    annotation = first_page_response['fullTextAnnotation']\n",
    "    # covert the json file to a bag of CSV lines\n",
    "    csv = \"\"\n",
    "    page_count = first_page\n",
    "    for resp in response.responses:\n",
    "        para_count = 0\n",
    "        for page in resp.full_text_annotation.pages:\n",
    "\n",
    "            # collect para features for the page\n",
    "            page_features = []\n",
    "            for block in page.blocks:\n",
    "                if str(block.block_type) != \"1\":  # process only TEXT blocks\n",
    "                    continue\n",
    "                for para in block.paragraphs:\n",
    "                    para_id = \"{}-{:03}-{:03}\".format(pdf_id, page_count, para_count)\n",
    "                    f = extract_paragraph_feature(para_id, para)\n",
    "                    page_features.append(f)\n",
    "                    para_count += 1\n",
    "\n",
    "            # output to csv\n",
    "            for f in page_features:\n",
    "                csv += '{},\"{}\",{},{:.6f},{:.6f},{:.6f},{:.6f},{:.6f},{:.6f},{:.6f},{}\\n'.format(\n",
    "                    f[\"para_id\"],\n",
    "                    f[\"text\"],\n",
    "                    f[\"chars\"],\n",
    "                    f[\"width\"],\n",
    "                    f[\"height\"],\n",
    "                    f[\"area\"],\n",
    "                    f[\"char_size\"],\n",
    "                    f[\"pos_x\"],\n",
    "                    f[\"pos_y\"],\n",
    "                    f[\"aspect\"],\n",
    "                    f[\"layout\"],\n",
    "                )\n",
    "\n",
    "        page_count += 1\n",
    "    return csv\n",
    "\n",
    "    # Here we print the full text from the first page.\n",
    "    # The response contains more information:\n",
    "    # annotation/pages/blocks/paragraphs/words/symbols\n",
    "    # including confidence scores and bounding boxes\n",
    "    print('Full text:\\n')\n",
    "    print(annotation['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Waiting for the operation to finish.\n",
      "Output files:\n",
      "Resume1 output-1-to-1.json\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'json_response' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-01315315ec52>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0masync_detect_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'gs://ocr_files_adil/resume.pdf'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'gs://ocr_files_adil/Resume1 '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-92c40467dc22>\u001b[0m in \u001b[0;36masync_detect_document\u001b[1;34m(gcs_source_uri, gcs_destination_uri)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0mcsv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[0mpage_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfirst_page\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mjson_response\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresponses\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m         \u001b[0mpara_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_text_annotation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpages\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'json_response' is not defined"
     ]
    }
   ],
   "source": [
    "async_detect_document('gs://ocr_files_adil/resume.pdf','gs://ocr_files_adil/Resume1 ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_csv(json_blob, pdf_id, first_page):\n",
    "\n",
    "    # parse json\n",
    "    json_string = json_blob.download_as_string()\n",
    "    json_response = json_format.Parse(json_string, vision.Enum.AnnotateFileResponse())\n",
    "\n",
    "    # covert the json file to a bag of CSV lines\n",
    "    csv = \"\"\n",
    "    page_count = first_page\n",
    "    for resp in json_response.responses:\n",
    "        para_count = 0\n",
    "        for page in resp.full_text_annotation.pages:\n",
    "\n",
    "            # collect para features for the page\n",
    "            page_features = []\n",
    "            for block in page.blocks:\n",
    "                if str(block.block_type) != \"1\":  # process only TEXT blocks\n",
    "                    continue\n",
    "                for para in block.paragraphs:\n",
    "                    para_id = \"{}-{:03}-{:03}\".format(pdf_id, page_count, para_count)\n",
    "                    f = extract_paragraph_feature(para_id, para)\n",
    "                    page_features.append(f)\n",
    "                    para_count += 1\n",
    "\n",
    "            # output to csv\n",
    "            for f in page_features:\n",
    "                csv += '{},\"{}\",{},{:.6f},{:.6f},{:.6f},{:.6f},{:.6f},{:.6f},{:.6f},{}\\n'.format(\n",
    "                    f[\"para_id\"],\n",
    "                    f[\"text\"],\n",
    "                    f[\"chars\"],\n",
    "                    f[\"width\"],\n",
    "                    f[\"height\"],\n",
    "                    f[\"area\"],\n",
    "                    f[\"char_size\"],\n",
    "                    f[\"pos_x\"],\n",
    "                    f[\"pos_y\"],\n",
    "                    f[\"aspect\"],\n",
    "                    f[\"layout\"],\n",
    "                )\n",
    "\n",
    "        page_count += 1\n",
    "    return csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_feature_csv('gs://ocr_files_adil/resume.pdf','gs://ocr_files_adil/Resume1 ',pdf_id, first_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = re.match(\"(.*).output-([0-9]+)-.*\",\"Resume1 output-1-to-1.json\")\n",
    "pdf_id = m.group(1)\n",
    "first_page = int(m.group(2))\n",
    "#json_blob = os.path.join(os.getcwd() , 'D:\\Projects\\google_ocr\\code\\My Project-b659c3bd30fa.json')\n",
    "json_blob=\"Resume1 output-1-to-1.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'download_as_string'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-bc1b73a5a0fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbuild_feature_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_blob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpdf_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfirst_page\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-dbff43c7c37b>\u001b[0m in \u001b[0;36mbuild_feature_csv\u001b[1;34m(json_blob, pdf_id, first_page)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# parse json\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mjson_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson_blob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload_as_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mjson_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEnum\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAnnotateFileResponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'download_as_string'"
     ]
    }
   ],
   "source": [
    "build_feature_csv(json_blob, pdf_id, first_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Very Importent Code\n",
    "from google.cloud import vision_v1\n",
    "\n",
    "\n",
    "def sample_batch_annotate_files(\n",
    "    storage_uri=\"gs://cloud-samples-data/vision/document_understanding/kafka.pdf\",\n",
    "):\n",
    "    \"\"\"Perform batch file annotation.\"\"\"\n",
    "    mime_type = \"application/pdf\"\n",
    "\n",
    "    client = vision_v1.ImageAnnotatorClient()\n",
    "\n",
    "    gcs_source = {\"uri\": storage_uri}\n",
    "    input_config = {\"gcs_source\": gcs_source, \"mime_type\": mime_type}\n",
    "    features = [{\"type_\": vision_v1.Feature.Type.DOCUMENT_TEXT_DETECTION}]\n",
    "\n",
    "    # The service can process up to 5 pages per document file.\n",
    "    # Here we specify the first, second, and last page of the document to be\n",
    "    # processed.\n",
    "    pages = [1, 2, -1]\n",
    "    requests = [{\"input_config\": input_config, \"features\": features, \"pages\": pages}]\n",
    "\n",
    "    response = client.batch_annotate_files(requests=requests)\n",
    "    for image_response in response.responses[0].responses:\n",
    "        print(u\"Full text: {}\".format(image_response.full_text_annotation.text))\n",
    "        for page in image_response.full_text_annotation.pages:\n",
    "            for block in page.blocks:\n",
    "                print(u\"\\nBlock confidence: {}\".format(block.confidence))\n",
    "                for par in block.paragraphs:\n",
    "                    print(u\"\\tParagraph confidence: {}\".format(par.confidence))\n",
    "                    for word in par.words:\n",
    "                        print(u\"\\t\\tWord confidence: {}\".format(word.confidence))\n",
    "                        for symbol in word.symbols:\n",
    "                            print(\n",
    "                                u\"\\t\\t\\tSymbol: {}, (confidence: {})\".format(\n",
    "                                    symbol.text, symbol.confidence\n",
    "                                )\n",
    "                            )\n"
   ]
  }
 ]
}